{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I. ETL Pipeline for Pre-Processing the Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PRE-PROCESSING THE FILES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Python packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Python packages \n",
    "import pandas as pd\n",
    "import cassandra\n",
    "import os\n",
    "import glob\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating list of filepaths to process original event csv data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/home/barghy/Documents/Analytics/data-engineering/projects/02-data-modelling-with-cassandra\n"
     ]
    }
   ],
   "source": [
    "# check current working directory\n",
    "print(os.getcwd())\n",
    "\n",
    "# get current folder and subfolder event data\n",
    "filepath = os.getcwd() + '/event_data'\n",
    "\n",
    "# create for loop to create a list of files and collect each filepath\n",
    "for root, dirs, files in os.walk(filepath):\n",
    "    \n",
    "# join the file path and roots with the subdirectories using glob\n",
    "    file_path_list = glob.glob(os.path.join(root,'*'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing the files to create the data file csv that will be used for Apache Casssandra tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "8056\n"
     ]
    }
   ],
   "source": [
    "# initiating an empty list of rows that will be generated from each file\n",
    "full_data_rows_list = [] \n",
    "    \n",
    "# for every filepath in the file path list \n",
    "for f in file_path_list:\n",
    "\n",
    "# reading csv file \n",
    "    with open(f, 'r', encoding = 'utf8', newline='') as csvfile: \n",
    "        # creating a csv reader object \n",
    "        csvreader = csv.reader(csvfile) \n",
    "        next(csvreader)\n",
    "        \n",
    " # extracting each data row one by one and append it    \n",
    "        for line in csvreader:\n",
    "            #print(line)\n",
    "            full_data_rows_list.append(line) \n",
    "            \n",
    "# get total number of rows \n",
    "print(len(full_data_rows_list))\n",
    "\n",
    "# create a smaller event data csv file called event_datafile_full csv that will be used to insert data into the \\\n",
    "# Apache Cassandra tables\n",
    "csv.register_dialect('myDialect', quoting=csv.QUOTE_ALL, skipinitialspace=True)\n",
    "\n",
    "with open('event_datafile_new.csv', 'w', encoding = 'utf8', newline='') as f:\n",
    "    writer = csv.writer(f, dialect='myDialect')\n",
    "    writer.writerow(['artist','firstName','gender','itemInSession','lastName','length',\\\n",
    "                'level','location','sessionId','song','userId'])\n",
    "    for row in full_data_rows_list:\n",
    "        if (row[0] == ''):\n",
    "            continue\n",
    "        writer.writerow((row[0], row[2], row[3], row[4], row[5], row[6], row[7], row[8], row[12], row[13], row[16]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "6821\n"
     ]
    }
   ],
   "source": [
    "# check the number of rows in csv file\n",
    "with open('event_datafile_new.csv', 'r', encoding = 'utf8') as f:\n",
    "    print(sum(1 for line in f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II. Apache Cassandra Non Relational Data Model\n",
    "\n",
    "## The event_datafile_new.csv contains the following columns: \n",
    "- artist \n",
    "- firstName (of user)\n",
    "- gender (of user)\n",
    "- itemInSession (order of song play)\n",
    "- lastName (of user)\n",
    "- length (of the song)\n",
    "- level (paid or free song)\n",
    "- location (of the user)\n",
    "- sessionId\n",
    "- song title\n",
    "- userId\n",
    "\n",
    "The image below is a screenshot of what the denormalized data looks like in the <font color=red>**event_datafile_new.csv**</font> after the files are processed:<br>\n",
    "\n",
    "<img src=\"images/image_event_datafile_new.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The code below defines the Apache Cassandra Data Model and ETL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a connection to the local Cassandra instance\n",
    "\n",
    "from cassandra.cluster import Cluster\n",
    "try: \n",
    "    cluster = Cluster(['127.0.0.1'])\n",
    "    session = cluster.connect()\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Keyspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Keyspace \n",
    "try:\n",
    "    session.execute(\"\"\"\n",
    "    CREATE KEYSPACE IF NOT EXISTS sparkify \n",
    "    WITH REPLICATION = \n",
    "    { 'class' : 'SimpleStrategy', 'replication_factor' : 1 }\"\"\"\n",
    ")\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Keyspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set KEYSPACE to sparkify\n",
    "try:\n",
    "    session.set_keyspace('udacity')\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Each query below coresponds to a question from the sparkify team. We follow the common pattern of creating one table per query to keep the data model modular and performant."
   ]
  },
  {
   "source": [
    "# Query 1: session_playlist\n",
    "\n",
    "### Give me the artist, song title and song's length in the music app history that was heard during  sessionId = 338, and itemInSession  = 4\n",
    "\n",
    "In this query, the primary key is composed of a partition key (sessionID) and a clustering key (itemInSession). The sessionID partitions and within that rows are ordered by itemInSession."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Query 1:  Give me the artist, song title and song's length in the music app history that was heard during \\\n",
    "## sessionId = 338, and itemInSession = 4\n",
    "\n",
    "query = \"CREATE TABLE IF NOT EXISTS session_playlist\"\n",
    "query = query + \"(sessionId int, itemInSession int,artist text, song text, length float, PRIMARY KEY (sessionID, itemInSession))\"\n",
    "\n",
    "try:\n",
    "    session.execute(query)\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# INSERT data into the session_playlist table from the .csv file\n",
    "\n",
    "file = 'event_datafile_new.csv'\n",
    "\n",
    "with open(file, encoding = 'utf8') as f:\n",
    "    csvreader = csv.reader(f)\n",
    "    next(csvreader) # skip header\n",
    "    for line in csvreader:\n",
    "        query = \"INSERT INTO session_playlist (sessionId, itemInSession, artist, song, length)\"\n",
    "        query = query + \" VALUES (%s, %s, %s, %s, %s)\"\n",
    "        \n",
    "        artist, firstName, gender, itemInSession, lastName, length, level, location, sessionId, song, userId = line\n",
    "\n",
    "        try:\n",
    "            session.execute(query, (int(sessionId), int(itemInSession), artist, song, float(length)))\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(e)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do a SELECT to verify that the data have been inserted into each table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "      artist                             song      length\n0  Faithless  Music Matters (Mark Knight Dub)  495.307312\n"
     ]
    }
   ],
   "source": [
    "## SELECT statement to verify the session_playlist table and query return the expected results\n",
    "query = \"SELECT artist, song, length FROM session_playlist WHERE sessionId = 338 AND itemInSession = 4\"\n",
    "\n",
    "try:\n",
    "    rows = session.execute(query)\n",
    "    df = pd.DataFrame(list(rows))\n",
    "    print(df)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query 2: user_session\n",
    "\n",
    "### Give me only the following: name of artist, song (sorted by itemInSession) and user (first and last name) for userid = 10, sessionid = 182\n",
    "\n",
    "In this query, the primary key is composed of a composite partition key (userId and sessionId) and a clustering key (ItemInSessionId). The userId and sessionId as a composite partitions and within that rows are ordered in ascending order by itemInSession."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TO-DO: Query 2: Give me only the following: name of artist, song (sorted by itemInSession) and user (first and last name)\\\n",
    "## for userid = 10, sessionid = 182\n",
    "\n",
    "query = \"CREATE TABLE IF NOT EXISTS user_session\"\n",
    "query = query + \"(userId int, sessionId int, artist text, song text, firstName text, lastName text, itemInSession int, PRIMARY KEY((userId, sessionId), itemInSession)) WITH CLUSTERING ORDER BY (itemInSession ASC)\"\n",
    "\n",
    "# ASC sorting is defaul - but included to be explicit\n",
    "\n",
    "try:\n",
    "    session.execute(query)\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSERT data into the user_session table from the .csv file\n",
    "\n",
    "file = 'event_datafile_new.csv'\n",
    "\n",
    "with open(file, encoding = 'utf8') as f:\n",
    "    csvreader = csv.reader(f)\n",
    "    next(csvreader) # skip header\n",
    "    for line in csvreader:\n",
    "        query = \"INSERT INTO user_session (userId, sessionId, artist, song, firstName, lastName, itemInSession)\"\n",
    "        query = query + \" VALUES (%s, %s, %s, %s, %s, %s, %s)\"\n",
    "        \n",
    "        artist, firstName, gender, itemInSession, lastName, length, level, location, sessionId, song, userId = line\n",
    "\n",
    "        try:\n",
    "            session.execute(query, (int(userId), int(sessionId), artist, song, firstName, lastName, int(itemInSession)))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "              artist                                               song  \\\n0   Down To The Bone                                 Keep On Keepin' On   \n1       Three Drives                                        Greece 2000   \n2  Sebastien Tellier                                          Kilometer   \n3      Lonnie Gordon  Catch You Baby (Steve Pitron & Max Sanna Radio...   \n\n  firstname lastname  \n0    Sylvie     Cruz  \n1    Sylvie     Cruz  \n2    Sylvie     Cruz  \n3    Sylvie     Cruz  \n"
     ]
    }
   ],
   "source": [
    "## SELECT statement to verify the user_session table and query return the expected results\n",
    "query = \"SELECT artist, song, firstName, lastName FROM user_session WHERE userId = 10 AND sessionId = 182\"\n",
    "\n",
    "try:\n",
    "    rows = session.execute(query)\n",
    "    df = pd.DataFrame(list(rows))\n",
    "    print(df)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "source": [
    "# Query 3: song_listeners\n",
    "\n",
    "### Give me every user name (first and last) in my music app history who listened to the song 'All Hands Against His Own'\n",
    "\n",
    "In this query, the primary key is composed of a partition key (song) and a clustering key (userId). The song partitions and within that rows are ordered by userId."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TO-DO: Query 3: Give me every user name (first and last) in my music app history who listened to the song 'All Hands Against His Own'\n",
    "\n",
    "query = \"CREATE TABLE IF NOT EXISTS song_listeners\"\n",
    "query = query + \"(song text, firstName text, lastName text, userId int, PRIMARY KEY(song, userId))\"\n",
    "\n",
    "try:\n",
    "    session.execute(query)\n",
    "\n",
    "except Exception as e:\n",
    "    print(e) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSERT data into the song_listeners table from the .csv file\n",
    "\n",
    "file = 'event_datafile_new.csv'\n",
    "\n",
    "with open(file, encoding = 'utf8') as f:\n",
    "    csvreader = csv.reader(f)\n",
    "    next(csvreader) # skip header\n",
    "    for line in csvreader:\n",
    "        query = \"INSERT INTO song_listeners (song, firstName, lastName, userId)\"\n",
    "        query = query + \" VALUES (%s, %s, %s, %s)\"\n",
    "        \n",
    "        artist, firstName, gender, itemInSession, lastName, length, level, location, sessionId, song, userId = line\n",
    "\n",
    "        try:\n",
    "            session.execute(query, (song, firstName, lastName, int(userId)))\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(e)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "    firstname lastname\n0  Jacqueline    Lynch\n1       Tegan   Levine\n2        Sara  Johnson\n"
     ]
    }
   ],
   "source": [
    "## SELECT statement to verify the song_listeners table and query return the expected results\n",
    "query = \"SELECT firstName, lastName FROM song_listeners WHERE song = 'All Hands Against His Own'\"\n",
    "\n",
    "try:\n",
    "    rows = session.execute(query)\n",
    "    df = pd.DataFrame(list(rows))\n",
    "    print(df)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop the tables before closing out the sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<cassandra.cluster.ResultSet at 0x7fd7f6f91a30>"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "## Drop the table before closing out the sessions\n",
    "\n",
    "session.execute(\"\"\"DROP TABLE session_playlist\"\"\")\n",
    "session.execute(\"\"\"DROP TABLE user_session\"\"\")\n",
    "session.execute(\"\"\"DROP TABLE song_listeners\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Close the session and cluster connectionÂ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Drop the table before closing out the sessions\n",
    "session.shutdown()\n",
    "cluster.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "ac7b6604438a2b71939ec90fa96767d7fe8d496fcb49eab870a18c747712c7ad"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}